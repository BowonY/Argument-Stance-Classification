{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Predict the topic labels of discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- We load all posts in each discussion, replace characters that are not numbers or [A-Za-z] by space, remove stopwords, perform stemming, and built unigram occurrence vector. \n",
    "- There are 98587 extracted unigrams, so you will see the vector of each discussion is quite sparse now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claasification Model\n",
    "- Train a SVM model with 75% stratified training data. \n",
    "- The testing accuracy is about 87%. \n",
    "- The SVM model (kernel='linear', Penalty parameter C=1) is saved as svm_model.pkl\n",
    "- Some additional dump files:\n",
    "    - unigram_dict.pickle: binary dump of unigram_dict\n",
    "    - unigram_dict.txt: list all unigrams\n",
    "    - discussions_unigram_label_dict.txt: unigram vectors of dicussions, the format of each row: \n",
    "    \"[Discussion ID],[Discussion Topic Label in text],[Discussion Unigram Occurance Vector]\"\n",
    "- The trained model can achieve higher than 90% accuracy when testing on all discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset files...\n",
      "5000 dicussions were loaded\n",
      "10000 dicussions were loaded\n",
      "11799 dicussions were loaded\n",
      "Loading topic file...\n",
      "Loading author stance file...\n",
      "===== Start preprocessing =====\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yang798/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Load unigram_dict ... \n",
      "Load discussions_unigram_label_dict ... \n",
      "1000 discussions were loaded\n",
      "2000 discussions were loaded\n",
      "2865 discussions were loaded\n",
      "===== Done! =====\n",
      "Divide data into train/test sets\n",
      "Load pre-trained SVM model\n",
      "Accuracy of all data: 0.97\n"
     ]
    }
   ],
   "source": [
    "import util.dataloader.IACDataLoader as iac\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "from sklearn import preprocessing, svm, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess(discussion_dict, topic_dict):\n",
    "    unigram_dict_path = \"unigram_dict.pickle\"\n",
    "    unigram_dict_text_path = \"unigram_dict.txt\"\n",
    "    discussions_unigram_label_dict_path = \"discussions_unigram_label_dict.txt\"\n",
    "    topic_list = sorted(topic_dict.keys())\n",
    "\n",
    "    print(\"===== Start preprocessing =====\")\n",
    "    # load stopwords list\n",
    "    nltk.download('stopwords')\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    # load Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Collect unigram list\n",
    "    unigram_dict = dict()\n",
    "    if not os.path.exists(unigram_dict_path):\n",
    "        print(\"Generate unigram_dict ... \")\n",
    "        topics = topic_dict.keys()\n",
    "        for topic in topics:\n",
    "            print(\"Start processing topic {}\".format(topic))\n",
    "            discussion_ids = topic_dict[topic]\n",
    "            idx = 0\n",
    "            for idx, discussion_id in enumerate(discussion_ids):\n",
    "                if (idx+1) % 100 == 0:\n",
    "                    print(\"{}/{} discussions were processed\".format(idx+1, len(discussion_ids)))\n",
    "                discussion = discussion_dict[discussion_id]\n",
    "                posts_text = discussion.get_posts_text()\n",
    "                for post_text in posts_text:\n",
    "                    # filter chars except words and numbers\n",
    "                    tmp_text = re.sub(r'[^\\w]', ' ', post_text)\n",
    "                    # filter stopwords and perform Porter stemmer\n",
    "                    tmp_text = ' '.join([stemmer.stem(word) for word in tmp_text.split() if word not in cachedStopWords])\n",
    "                    words = tmp_text.split(' ')\n",
    "                    for word in words:\n",
    "                        if word in unigram_dict:\n",
    "                            unigram_dict[word] += 1\n",
    "                        else:\n",
    "                            unigram_dict[word] = 1\n",
    "            print(\"{}/{} discussions were processed\".format(idx+1, len(discussion_ids)))\n",
    "        pickle.dump(unigram_dict, open(unigram_dict_path, \"wb\"))\n",
    "        with open(unigram_dict_text_path, \"w\") as output:\n",
    "            for key in unigram_dict.keys():\n",
    "                output.write(\"{}\\t{}\\n\".format(key, unigram_dict[key]))\n",
    "    else:\n",
    "        print(\"Load unigram_dict ... \")\n",
    "        unigram_dict = pickle.load(open(unigram_dict_path, \"rb\"))\n",
    "\n",
    "    # Generate unigram vector for discussions\n",
    "    X = []\n",
    "    y = []\n",
    "    unigram_list = sorted(unigram_dict.keys())\n",
    "    with open(\"unigram_list.txt\", \"w\") as output:\n",
    "        for u in unigram_list:\n",
    "            output.write(\"{}\\n\".format(u))\n",
    "    discussions_unigram_label_dict = dict()\n",
    "    if not os.path.exists(discussions_unigram_label_dict_path):\n",
    "        print(\"Generate unigram vector for discussions ...\")\n",
    "        topics = topic_dict.keys()\n",
    "        for topic in topics:\n",
    "            print(\"Start processing topic {}\".format(topic))\n",
    "            discussion_ids = topic_dict[topic]\n",
    "            idx = 0\n",
    "            for idx, discussion_id in enumerate(discussion_ids):\n",
    "                if (idx+1) % 10 == 0:\n",
    "                    print(\"{}/{} discussions were processed\".format(idx+1, len(discussion_ids)))\n",
    "                word_dict = dict()\n",
    "                discussion = discussion_dict[discussion_id]\n",
    "                posts_text = discussion.get_posts_text()\n",
    "                for post_text in posts_text:\n",
    "                    # filter chars except words and numbers\n",
    "                    tmp_text = re.sub(r'[^\\w]', ' ', post_text)\n",
    "                    # filter stopwords and perform Porter stemmer\n",
    "                    tmp_text = ' '.join([stemmer.stem(word) for word in tmp_text.split() if word not in cachedStopWords])\n",
    "                    words = tmp_text.split(' ')\n",
    "                    for word in words:\n",
    "                        if word in word_dict:\n",
    "                            word_dict[word] += 1\n",
    "                        else:\n",
    "                            word_dict[word] = 1\n",
    "                unigram_vec = [0] * len(unigram_list)\n",
    "                w_keys = word_dict.keys()\n",
    "                for w_key in w_keys:\n",
    "                    unigram_vec[unigram_list.index(w_key)] = word_dict[w_key]\n",
    "                discussions_unigram_label_dict[discussion_id] = [unigram_vec, topic]\n",
    "            print(\"{}/{} discussions were processed\".format(idx+1, len(discussion_ids)))\n",
    "        with open(\"discussions_unigram_label_dict.txt\", \"w\") as output:\n",
    "            for d_id in discussions_unigram_label_dict.keys():\n",
    "                vec, label = discussions_unigram_label_dict[d_id]\n",
    "                vec_str = ','.join([str(v) for v in vec])\n",
    "                output.write(\"{},{},{}\\n\".format(d_id, label, vec_str))\n",
    "\n",
    "        with open(\"discussions_unigram_label_dict.txt\", \"r\") as f:\n",
    "            idx = 0\n",
    "            for idx, line in enumerate(f):\n",
    "                if (idx+1)%1000 == 0:\n",
    "                    print(\"{} discussions were loaded\".format(idx+1))\n",
    "                # if idx>=300:\n",
    "                #     break\n",
    "                items = line.strip().split(',')\n",
    "                X.append(items[2:])\n",
    "                y.append(topic_list.index(items[1]))\n",
    "            print(\"{} discussions were loaded\".format(idx))\n",
    "    else:\n",
    "        print(\"Load discussions_unigram_label_dict ... \")\n",
    "        with open(\"discussions_unigram_label_dict.txt\", \"r\") as f:\n",
    "            idx = 0\n",
    "            for idx, line in enumerate(f):\n",
    "                if (idx+1)%1000 == 0:\n",
    "                    print(\"{} discussions were loaded\".format(idx+1))\n",
    "                # if idx>=300:\n",
    "                #     break\n",
    "                items = line.strip().split(',')\n",
    "                X.append(items[2:])\n",
    "                y.append(topic_list.index(items[1]))\n",
    "            print(\"{} discussions were loaded\".format(idx))\n",
    "\n",
    "    print(\"===== Done! =====\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataloader = iac.IACDataLoader()\n",
    "    dataloader.set_dataset_dir(\"dataset/discussions\")\n",
    "    dataloader.set_topic_filepath(\"dataset/topic.csv\")\n",
    "    dataloader.set_stance_filepath(\"dataset/author_stance.csv\")\n",
    "    dataloader.load()\n",
    "    topic_dict = dataloader.get_topic_dict()\n",
    "    discussion_dict = dataloader.get_discussion_dict()\n",
    "    author_stance_dict = dataloader.get_author_stance_dict()\n",
    "\n",
    "    topic_list = sorted(topic_dict.keys())\n",
    "\n",
    "    X, y = preprocess(discussion_dict, topic_dict)\n",
    "\n",
    "    print(\"Divide data into train/test sets\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25)\n",
    "    svm_model_path = \"svm_model.pkl\"\n",
    "    clf = None\n",
    "    if not os.path.exists(svm_model_path):\n",
    "        print(\"Initialize SVM model\")\n",
    "        clf = svm.SVC(kernel='linear', C=1)\n",
    "        print(\"Fit the model\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\"Accuracy of testing data: {}\".format(clf.score(X_test, y_test)))\n",
    "        print(\"Dump the model...\")\n",
    "        joblib.dump(clf, open(svm_model_path, \"wb\"))\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"Load pre-trained SVM model\")\n",
    "        clf = joblib.load(open(svm_model_path, \"rb\"))\n",
    "\n",
    "    if clf:\n",
    "        print(\"Accuracy of all data: {:.2f}\".format(clf.score(X, y)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
